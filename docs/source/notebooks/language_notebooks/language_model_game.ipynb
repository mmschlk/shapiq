{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b000618e37afdb3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Explaining a language model for sentiment analysis\n",
    "\n",
    "This notebook shows how we can use `shapiq` to explain the predictions of a language sentiment analysis model. For that, we will create a custom *game* that will be used for the explanation. The benchmark game resulting from this tutorial is available as `shapiq.games.SentimentClassificationGame`.\n",
    "\n",
    "First, we need to install the required packages next to `shapiq`. We will use a language model from the `transformers` library; specifically relying on `torch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96756a5298128aed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T15:20:25.892501Z",
     "start_time": "2025-07-11T15:20:25.771640Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: pip\r\n"
     ]
    }
   ],
   "source": [
    "# Install the required packages\n",
    "!pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "233a68eadd33ade3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T11:36:59.447126Z",
     "start_time": "2025-09-09T11:36:43.186224Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapiq version: None\n"
     ]
    }
   ],
   "source": [
    "# Import the required libraries\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "\n",
    "import shapiq\n",
    "\n",
    "print(f\"shapiq version: {shapiq.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f9a6a38b3b0214",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Language model\n",
    "We will use a pre-trained BERT model for sentiment analysis. We will use the `transformers` library to load the model and tokenizer. We will use the `lvwerra/distilbert-imdb` model for this tutorial.\n",
    "\n",
    "The model predicts the sentiment of the sentence as **positive**. For this model (and other sentiment-analysis models), the output is a list of dictionaries, where each dictionary contains the `label` and the `score` of the sentiment. The label can be either `POSITIVE` or `NEGATIVE`. The score is the probability of the sentiment being positive or negative. The tokenized sentence contains the tokens of the sentence. The special tokens map contains the special tokens used by the model. We will need the `mask_token` later in the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50f59cc77301eef0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T11:37:01.836492Z",
     "start_time": "2025-09-09T11:36:59.460475Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier output: [{'label': 'POSITIVE', 'score': 0.9951981902122498}]\n",
      "Tokenized sentence: {'input_ids': [101, 1045, 2293, 2023, 3185, 999, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n",
      "Special tokens: {'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n",
      "Mask token id: 103\n"
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer\n",
    "classifier = pipeline(task=\"sentiment-analysis\", model=\"lvwerra/distilbert-imdb\")\n",
    "tokenizer = classifier.tokenizer\n",
    "\n",
    "test_sentence = \"I love this movie!\"\n",
    "print(f\"Classifier output: {classifier(test_sentence)}\")\n",
    "\n",
    "tokenized_sentence = tokenizer(test_sentence)\n",
    "print(f\"Tokenized sentence: {tokenized_sentence}\")\n",
    "\n",
    "special_tokens = tokenizer.special_tokens_map\n",
    "print(f\"Special tokens: {tokenizer.special_tokens_map}\")\n",
    "\n",
    "mask_toke_id = tokenizer.mask_token_id\n",
    "print(f\"Mask token id: {mask_toke_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e75bdac10f7042",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can inspect the behavior of the model by checking the output of the classifier for different sentences and by decoding the tokenized sentences. The `tokenizer.decode` function can be used to decode the tokenized sentence. The `[CLS]` token is used to mark the beginning of the sentence, and the `[SEP]` token is used to mark the end of the sentence. Notice that also the `!` token is tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3b3b6f4193e7d73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T11:37:01.907115Z",
     "start_time": "2025-09-09T11:37:01.905202Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded sentence: [CLS] i love this movie! [SEP]\n",
      "Decoded sentence: i love this movie! - Tokenized input: [1045 2293 2023 3185  999] - 5 tokens.\n"
     ]
    }
   ],
   "source": [
    "# Test the tokenizer\n",
    "decoded_sentence = tokenizer.decode(tokenized_sentence[\"input_ids\"])\n",
    "print(f\"Decoded sentence: {decoded_sentence}\")\n",
    "\n",
    "# Remove the start and end tokens\n",
    "tokenized_input = np.asarray(tokenizer(test_sentence)[\"input_ids\"][1:-1])\n",
    "decoded_sentence = tokenizer.decode(tokenized_input)\n",
    "print(\n",
    "    f\"Decoded sentence: {decoded_sentence} - Tokenized input: {tokenized_input} - {len(tokenized_input)} tokens.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97381c1da32a6c49",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Since the start and end tokens are always present this information is not relevant for our explanation. To explain this classifier we need to model its behavior as a cooperative game."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca96f0af12688",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Treating the language model as a game with a value function\n",
    "For all Shapley-based feature attribution methods, we need to model the problem as a cooperative game. We need to define a **value function** that assigns a real-valued worth to each coalition of features. In this case, the features are the tokens of the sentence (without the `[CLS]` and `[SEP]` tokens). The value of the coalition is the sentiment score of the sentence with tokens that are not participating in the coalition `masked` or `removed`.\n",
    "\n",
    "A value function has the following formal definition:\n",
    "$$v: 2^N \\rightarrow \\mathbb{R}$$\n",
    "where $N$ is the set of features (tokens in our case). \n",
    "\n",
    "To be able to model `POSITIVE` and `NEGATIVE` sentiments, we need to map the output of the classifier to be in the range $[-1, 1]$. We can do this with the following function which accepts a list of input texts and returns a vector of the sentiment of the input texts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bce879ce457e9a98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T11:37:01.936187Z",
     "start_time": "2025-09-09T11:37:01.914376Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model call: [ 0.99519819 -0.95526284]\n"
     ]
    }
   ],
   "source": [
    "# Define the model call function\n",
    "def model_call(input_texts: list[str]) -> np.ndarray[float]:\n",
    "    \"\"\"Calls the sentiment classification model with a list of texts.\n",
    "\n",
    "    Args:\n",
    "        input_texts: A list of input texts.\n",
    "\n",
    "    Returns:\n",
    "        A vector of the sentiment of the input texts.\n",
    "    \"\"\"\n",
    "    outputs = classifier(input_texts)\n",
    "    outputs = [\n",
    "        output[\"score\"] * 1 if output[\"label\"] == \"POSITIVE\" else output[\"score\"] * -1\n",
    "        for output in outputs\n",
    "    ]\n",
    "    return np.array(outputs, dtype=float)\n",
    "\n",
    "\n",
    "# Test the model call function\n",
    "print(f\"Model call: {model_call(['I love this movie!', 'I hate this movie!'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee183b3800498675",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "With this model call function, we can now define the value function. In our world the value function accepts one-hot-encoded numpy matrices denoting the coalitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d176905292347ec1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T11:37:02.004695Z",
     "start_time": "2025-09-09T11:37:02.002640Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty coalition: [[False False False False False]]\n",
      "Full coalition: [[ True  True  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "# Show coalitions\n",
    "n_players = len(tokenized_sentence[\"input_ids\"]) - 2  # remove [CLS] and [SEP]\n",
    "\n",
    "empty_coalition = np.zeros((1, n_players), dtype=bool)  # empty coalition\n",
    "full_coalition = np.ones((1, n_players), dtype=bool)  # full coalition\n",
    "\n",
    "print(f\"Empty coalition: {empty_coalition}\")\n",
    "print(f\"Full coalition: {full_coalition}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8100b1a3fc09e3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "With these coalitions we can now define the value function. However, for most algorithms it is important that the value function is normalized (also known as centered). This means that the value of the empty coalition is 0. We can achieve this by subtracting the value of the empty coalition from the value of the coalition. This is done in the `shapiq` library, but we can also do it here.\n",
    "\n",
    "Formally, the normalized value function is defined as:\n",
    "$$v_0 := v(S) - v(\\emptyset)$$\n",
    "where $v(S)$ is the value of the coalition $S$ and $v(\\emptyset)$ is the value of the empty coalition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79a5c423622a0904",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T11:37:02.045609Z",
     "start_time": "2025-09-09T11:37:02.043439Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the value function\n",
    "def value_function(\n",
    "    coalitions: np.ndarray[bool], tokenized_input: np.ndarray[int], normalization_value: float = 0.0\n",
    ") -> np.ndarray[float]:\n",
    "    \"\"\"Computes the value of the coalitions.\n",
    "\n",
    "    Args:\n",
    "        coalitions: A numpy matrix of shape (n_coalitions, n_players).\n",
    "        tokenized_input: A numpy array of the tokenized input sentence.\n",
    "        normalization_value: The value of the empty coalition. Default is 0.0 (no normalization).\n",
    "\n",
    "    Returns:\n",
    "        A vector of the value of the coalitions.\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    for coalition in coalitions:\n",
    "        tokenized_coalition = tokenized_input.copy()\n",
    "        # all tokens not in the coalition are set to mask_token_id\n",
    "        tokenized_coalition[~coalition] = mask_toke_id\n",
    "        coalition_text = tokenizer.decode(tokenized_coalition)\n",
    "        texts.append(coalition_text)\n",
    "\n",
    "    # get the sentiment of the texts (call the model as defined above)\n",
    "    sentiments = model_call(texts)\n",
    "\n",
    "    # normalize/center the value function\n",
    "    return sentiments - normalization_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b971656158325b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can test the value function without normalization. The output of the value function for the grand coalition (full coalition) should be the same as the output of the classifier. The output of the value function for the empty coalition is some bias value in the model which often is not zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22b2201ca139c0d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T11:37:02.090971Z",
     "start_time": "2025-09-09T11:37:02.062259Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of the classifier: [{'label': 'POSITIVE', 'score': 0.9951981902122498}]\n",
      "Value function for the full coalition: 0.9951981902122498\n",
      "Value function for the empty coalition: 0.5192136764526367\n"
     ]
    }
   ],
   "source": [
    "# Test the value function without normalization\n",
    "print(f\"Output of the classifier: {classifier(test_sentence)}\")\n",
    "\n",
    "print(\n",
    "    f\"Value function for the full coalition: {value_function(full_coalition, tokenized_input=tokenized_input)[0]}\"\n",
    ")\n",
    "print(\n",
    "    f\"Value function for the empty coalition: {value_function(empty_coalition, tokenized_input=tokenized_input)[0]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae20674fc899a202",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "If we normalize the value function, the output of the value function for the empty coalition should be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "338e1ae439120652",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T11:37:10.450801Z",
     "start_time": "2025-09-09T11:37:10.396366Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function for the full coalition: 0.47598451375961304\n",
      "Value function for the empty coalition: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Test the value function with normalization\n",
    "normalization_value = float(value_function(empty_coalition, tokenized_input=tokenized_input)[0])\n",
    "print(\n",
    "    f\"Value function for the full coalition: {value_function(full_coalition, tokenized_input=tokenized_input, normalization_value=normalization_value)[0]}\"\n",
    ")\n",
    "print(\n",
    "    f\"Value function for the empty coalition: {value_function(empty_coalition, tokenized_input=tokenized_input, normalization_value=normalization_value)[0]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be865dbf772ea6c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "`shapiq` expects the game to be only dependent on the coalitions. For this we can write a small wrapper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91e8b195226e1ecb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T11:37:12.932553Z",
     "start_time": "2025-09-09T11:37:12.888455Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game for the full coalition: 0.47598451375961304\n",
      "Game for the empty coalition: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Define the game function\n",
    "def game_fun(coalitions: np.ndarray[bool]) -> np.ndarray[float]:\n",
    "    \"\"\"Wrapper function for the value function.\n",
    "\n",
    "    Args:\n",
    "        coalitions: A numpy matrix of shape (n_coalitions, n_players).\n",
    "\n",
    "    Returns:\n",
    "        A vector of the value of the coalitions.\n",
    "    \"\"\"\n",
    "    return value_function(\n",
    "        coalitions, tokenized_input=tokenized_input, normalization_value=normalization_value\n",
    "    )\n",
    "\n",
    "\n",
    "# Test the game function\n",
    "print(f\"Game for the full coalition: {game_fun(full_coalition)[0]}\")\n",
    "print(f\"Game for the empty coalition: {game_fun(empty_coalition)[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5762eda66918ae03",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can use this callable already in `shapiq`, but we can also define it as a proper `Game` object, which comes with some additional functionality. Notice that the `value_function` function is now a method of the `SentimentClassificationGame` class and you do not have to worry about the normalization. This is done automatically by the `Game` class which also contains the `__call__` method meaning that this class is also callable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea94eb7697abad0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T11:41:12.599050Z",
     "start_time": "2025-09-09T11:41:08.475462Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game for the full coalition: 0.42618465423583984\n",
      "Game for the empty coalition: 0.0\n",
      "Number of players: 14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "486570f24266473888d7e83e157c6a0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating game:   0%|          | 0/1000 [00:00<?, ? coalition/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{(): 0.0,\n",
       " (0,): 0.1281850683357642,\n",
       " (1,): 0.16477675362353933,\n",
       " (2,): -0.10427398467396792,\n",
       " (3,): 0.042260688925211516,\n",
       " (4,): 0.4719322061886541,\n",
       " (5,): -0.06971901708938133,\n",
       " (6,): 0.00614513468790389,\n",
       " (7,): -0.027581892984435367,\n",
       " (8,): -0.07605197781749666,\n",
       " (9,): -0.048323969300758385,\n",
       " (10,): 0.020283191763113134,\n",
       " (11,): 0.01437002822195331,\n",
       " (12,): 0.09009149133469221,\n",
       " (13,): -0.679034900468855,\n",
       " (0, 1): -0.1700896502130735,\n",
       " (0, 2): 0.14428933585967288,\n",
       " (0, 3): -0.029010846900615415,\n",
       " (0, 4): -0.20728601831518917,\n",
       " (0, 5): 0.09693795345421462,\n",
       " (0, 6): -0.02519621304461267,\n",
       " (0, 7): 0.011370324706313157,\n",
       " (0, 8): 0.014651396032848638,\n",
       " (0, 9): 0.04171570829839341,\n",
       " (0, 10): 0.03158443766161025,\n",
       " (0, 11): -0.016208905071481712,\n",
       " (0, 12): -0.043916084052443786,\n",
       " (0, 13): 0.11830847012496037,\n",
       " (1, 2): 0.17951830512242056,\n",
       " (1, 3): -0.016306144009029765,\n",
       " (1, 4): -0.319322724685267,\n",
       " (1, 5): 0.09645752130939786,\n",
       " (1, 6): -0.0043827111297652915,\n",
       " (1, 7): -0.0012296228953853979,\n",
       " (1, 8): 0.08369991028411773,\n",
       " (1, 9): 0.052111809547205734,\n",
       " (1, 10): 0.02599168747910439,\n",
       " (1, 11): -0.03387085602706483,\n",
       " (1, 12): -0.030644853973624882,\n",
       " (1, 13): 0.1417772949473026,\n",
       " (2, 3): 0.10469895442885534,\n",
       " (2, 4): 0.08087425866142312,\n",
       " (2, 5): -0.1418544672784606,\n",
       " (2, 6): -0.16217914216839335,\n",
       " (2, 7): -0.051226196914265415,\n",
       " (2, 8): -0.1658763565052954,\n",
       " (2, 9): 0.0006587583579216069,\n",
       " (2, 10): 0.04202044652612292,\n",
       " (2, 11): 0.0006736609400973468,\n",
       " (2, 12): -0.022860170671179507,\n",
       " (2, 13): 0.046541452946674756,\n",
       " (3, 4): -0.1302688966888922,\n",
       " (3, 5): 0.03441631788994716,\n",
       " (3, 6): 0.06933322791698023,\n",
       " (3, 7): 0.01849541924360733,\n",
       " (3, 8): 0.00660755765566713,\n",
       " (3, 9): 0.0518205619261186,\n",
       " (3, 10): -0.018531468270240617,\n",
       " (3, 11): 0.0052090761824813096,\n",
       " (3, 12): -0.013994359477483113,\n",
       " (3, 13): -0.03027015902085028,\n",
       " (4, 5): 0.0677599875467862,\n",
       " (4, 6): -0.005066976485410061,\n",
       " (4, 7): 0.03272510980755201,\n",
       " (4, 8): 0.10873466272707742,\n",
       " (4, 9): 0.023303399413293065,\n",
       " (4, 10): -0.06694518488527475,\n",
       " (4, 11): -0.014943430774896899,\n",
       " (4, 12): -0.07622347617591194,\n",
       " (4, 13): 0.1822247640496816,\n",
       " (5, 6): 0.03871010662542366,\n",
       " (5, 7): -0.11725073907772056,\n",
       " (5, 8): -0.05783301380226677,\n",
       " (5, 9): -0.07248576051036253,\n",
       " (5, 10): -0.011883157493766831,\n",
       " (5, 11): 0.02369685536902667,\n",
       " (5, 12): 0.017081060632081875,\n",
       " (5, 13): 0.09124173116010584,\n",
       " (6, 7): 0.01766965745078787,\n",
       " (6, 8): 0.05323930789902388,\n",
       " (6, 9): 0.022644993275732786,\n",
       " (6, 10): 0.023795551214372223,\n",
       " (6, 11): 0.03725689932225767,\n",
       " (6, 12): -0.07667632638800441,\n",
       " (6, 13): 0.035703242861273646,\n",
       " (7, 8): 0.03529412439703303,\n",
       " (7, 9): 0.0196620761594126,\n",
       " (7, 10): -0.02889137855861167,\n",
       " (7, 11): -0.04813678092437143,\n",
       " (7, 12): 0.05158424218560106,\n",
       " (7, 13): 0.08231626124891772,\n",
       " (8, 9): -0.04594429543782506,\n",
       " (8, 10): 0.01791360402200905,\n",
       " (8, 11): -0.035941199909617,\n",
       " (8, 12): 0.02376080553483084,\n",
       " (8, 13): -0.014924971684350662,\n",
       " (9, 10): -0.067240273822299,\n",
       " (9, 11): 0.0028430610736718464,\n",
       " (9, 12): 0.016218859487472398,\n",
       " (9, 13): -0.01007498300268463,\n",
       " (10, 11): 0.006625733738628506,\n",
       " (10, 12): -0.05174597790442738,\n",
       " (10, 13): 0.10651298789534785,\n",
       " (11, 12): -0.040211853474273536,\n",
       " (11, 13): 0.1394984841155285,\n",
       " (12, 13): 0.19229004542951766}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SentimentClassificationGame(shapiq.Game):\n",
    "    \"\"\"The sentiment analysis classifier modeled as a cooperative game.\n",
    "\n",
    "    Args:\n",
    "        classifier: The sentiment analysis classifier.\n",
    "        tokenizer: The tokenizer of the classifier.\n",
    "        test_sentence: The sentence to be explained.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, classifier, tokenizer, test_sentence: str) -> None:\n",
    "        self.classifier = classifier\n",
    "        self.tokenizer = tokenizer\n",
    "        self.test_sentence = test_sentence\n",
    "        self.mask_token_id = tokenizer.mask_token_id\n",
    "        self.tokenized_input = np.asarray(tokenizer(test_sentence)[\"input_ids\"][1:-1])\n",
    "        self.n_players = len(self.tokenized_input)\n",
    "\n",
    "        empty_coalition = np.zeros((1, len(self.tokenized_input)), dtype=bool)\n",
    "        self.normalization_value = float(self.value_function(empty_coalition)[0])\n",
    "        super().__init__(n_players=self.n_players, normalization_value=self.normalization_value)\n",
    "\n",
    "    def value_function(self, coalitions: np.ndarray[bool]) -> np.ndarray[float]:\n",
    "        \"\"\"Computes the value of the coalitions.\n",
    "\n",
    "        Args:\n",
    "            coalitions: A numpy matrix of shape (n_coalitions, n_players).\n",
    "\n",
    "        Returns:\n",
    "            A vector of the value of the coalitions.\n",
    "        \"\"\"\n",
    "        texts = []\n",
    "        for coalition in coalitions:\n",
    "            tokenized_coalition = self.tokenized_input.copy()\n",
    "            # all tokens not in the coalition are set to mask_token_id\n",
    "            tokenized_coalition[~coalition] = self.mask_token_id\n",
    "            coalition_text = self.tokenizer.decode(tokenized_coalition)\n",
    "            texts.append(coalition_text)\n",
    "\n",
    "        # get the sentiment of the texts (call the model as defined above)\n",
    "        return self._model_call(texts)\n",
    "\n",
    "    def _model_call(self, input_texts: list[str]) -> np.ndarray[float]:\n",
    "        \"\"\"Calls the sentiment classification model with a list of texts.\n",
    "\n",
    "        Args:\n",
    "            input_texts: A list of input texts.\n",
    "\n",
    "        Returns:\n",
    "            A vector of the sentiment of the input texts.\n",
    "        \"\"\"\n",
    "        outputs = self.classifier(input_texts)\n",
    "        outputs = [\n",
    "            output[\"score\"] * 1 if output[\"label\"] == \"POSITIVE\" else output[\"score\"] * -1\n",
    "            for output in outputs\n",
    "        ]\n",
    "        return np.array(outputs, dtype=float)\n",
    "\n",
    "\n",
    "test_sentence = \"I really like this amazing movie by Dr. Fumagalli!\"\n",
    "# Test SentimentClassificationGame\n",
    "game_class = SentimentClassificationGame(classifier, tokenizer, test_sentence)\n",
    "print(f\"Game for the full coalition: {game_class(game_class.grand_coalition)[0]}\")\n",
    "print(f\"Game for the empty coalition: {game_class(game_class.empty_coalition)[0]}\")\n",
    "print(f\"Number of players: {game_class.n_players}\")\n",
    "game_class.verbose = True\n",
    "\n",
    "approximator = shapiq.KernelSHAPIQ(n=game_class.n_players, max_order=2, index=\"k-SII\")\n",
    "sii_values = approximator.approximate(budget=1000, game=game_class)\n",
    "sii_values.dict_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a100294487e50fc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Computing Shapley interactions\n",
    "We can now use the `game_fun` function or the `SentimentClassificationGame` class to compute the Shapley interactions with methods provided in `shapiq`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f62adc49538c8a79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T11:40:23.311592Z",
     "start_time": "2025-09-09T11:40:11.211633Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Compute Shapley interactions with the ShapIQ approximator for the game function\u001b[39;00m\n\u001b[1;32m      2\u001b[0m approximator \u001b[38;5;241m=\u001b[39m shapiq\u001b[38;5;241m.\u001b[39mKernelSHAPIQ(n\u001b[38;5;241m=\u001b[39mgame_class\u001b[38;5;241m.\u001b[39mn_players, max_order\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk-SII\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m sii_values \u001b[38;5;241m=\u001b[39m \u001b[43mapproximator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapproximate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbudget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgame_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_players\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgame\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgame_class\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m sii_values\u001b[38;5;241m.\u001b[39mdict_values\n",
      "File \u001b[0;32m~/PycharmProjects/shapiq/src/shapiq/approximator/regression/base.py:155\u001b[0m, in \u001b[0;36mRegression.approximate\u001b[0;34m(self, budget, game, *args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler\u001b[38;5;241m.\u001b[39msample(budget)\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# query the game for the coalitions\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m game_values \u001b[38;5;241m=\u001b[39m \u001b[43mgame\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sampler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoalitions_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapproximation_index \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSII\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sii_consistent:\n\u001b[1;32m    158\u001b[0m     shapley_interactions_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_shap_iq_routine(\n\u001b[1;32m    159\u001b[0m         kernel_weights_dict\u001b[38;5;241m=\u001b[39mkernel_weights_dict,\n\u001b[1;32m    160\u001b[0m         game_values\u001b[38;5;241m=\u001b[39mgame_values,\n\u001b[1;32m    161\u001b[0m     )\n",
      "File \u001b[0;32m~/PycharmProjects/shapiq/src/shapiq/game.py:445\u001b[0m, in \u001b[0;36mGame.__call__\u001b[0;34m(self, coalitions, verbose)\u001b[0m\n\u001b[1;32m    443\u001b[0m verbose \u001b[38;5;241m=\u001b[39m verbose \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecomputed \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m verbose:\n\u001b[0;32m--> 445\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoalitions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecomputed \u001b[38;5;129;01mand\u001b[39;00m verbose:\n\u001b[1;32m    447\u001b[0m     values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(coalitions\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m)\n",
      "Cell \u001b[0;32mIn [14], line 40\u001b[0m, in \u001b[0;36mSentimentClassificationGame.value_function\u001b[0;34m(self, coalitions)\u001b[0m\n\u001b[1;32m     37\u001b[0m     texts\u001b[38;5;241m.\u001b[39mappend(coalition_text)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# get the sentiment of the texts (call the model as defined above)\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [14], line 51\u001b[0m, in \u001b[0;36mSentimentClassificationGame._model_call\u001b[0;34m(self, input_texts)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_model_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_texts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m     43\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Calls the sentiment classification model with a list of texts.\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m        A vector of the sentiment of the input texts.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_texts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     53\u001b[0m         output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOSITIVE\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs\n\u001b[1;32m     55\u001b[0m     ]\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(outputs, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m)\n",
      "File \u001b[0;32m~/PycharmProjects/shapiq/.venv/lib/python3.11/site-packages/transformers/pipelines/text_classification.py:159\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;124;03mClassify the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m    If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    158\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (inputs,)\n\u001b[0;32m--> 159\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m _legacy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n",
      "File \u001b[0;32m~/PycharmProjects/shapiq/.venv/lib/python3.11/site-packages/transformers/pipelines/base.py:1352\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1349\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   1350\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1351\u001b[0m     )\n\u001b[0;32m-> 1352\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(final_iterator)\n\u001b[1;32m   1353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[1;32m   1354\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/PycharmProjects/shapiq/.venv/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/PycharmProjects/shapiq/.venv/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py:125\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m    124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[0;32m--> 125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[0;32m~/PycharmProjects/shapiq/.venv/lib/python3.11/site-packages/transformers/pipelines/base.py:1278\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1276\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1277\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1278\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1279\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1280\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/PycharmProjects/shapiq/.venv/lib/python3.11/site-packages/transformers/pipelines/text_classification.py:190\u001b[0m, in \u001b[0;36mTextClassificationPipeline._forward\u001b[0;34m(self, model_inputs)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(model_forward)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    189\u001b[0m     model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/shapiq/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/shapiq/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/PycharmProjects/shapiq/.venv/lib/python3.11/site-packages/transformers/models/distilbert/modeling_distilbert.py:977\u001b[0m, in \u001b[0;36mDistilBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    970\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    975\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 977\u001b[0m distilbert_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistilbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    986\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m distilbert_output[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[1;32m    987\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m hidden_state[:, \u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, dim)\u001b[39;00m\n",
      "File \u001b[0;32m~/PycharmProjects/shapiq/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/shapiq/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/PycharmProjects/shapiq/.venv/lib/python3.11/site-packages/transformers/models/distilbert/modeling_distilbert.py:797\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_sdpa \u001b[38;5;129;01mand\u001b[39;00m head_mask_is_none \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_attentions:\n\u001b[1;32m    793\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m _prepare_4d_attention_mask_for_sdpa(\n\u001b[1;32m    794\u001b[0m             attention_mask, embeddings\u001b[38;5;241m.\u001b[39mdtype, tgt_len\u001b[38;5;241m=\u001b[39minput_shape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    795\u001b[0m         )\n\u001b[0;32m--> 797\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/shapiq/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/shapiq/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/PycharmProjects/shapiq/.venv/lib/python3.11/site-packages/transformers/models/distilbert/modeling_distilbert.py:550\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    542\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    543\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    544\u001b[0m         hidden_state,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         output_attentions,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 550\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    557\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/PycharmProjects/shapiq/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/shapiq/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/PycharmProjects/shapiq/.venv/lib/python3.11/site-packages/transformers/models/distilbert/modeling_distilbert.py:476\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;124;03mParameters:\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;124;03m    x: torch.tensor(bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m    torch.tensor(bs, seq_length, dim) The output of the transformer block contextualization.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;66;03m# Self-Attention\u001b[39;00m\n\u001b[0;32m--> 476\u001b[0m sa_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    485\u001b[0m     sa_output, sa_weights \u001b[38;5;241m=\u001b[39m sa_output  \u001b[38;5;66;03m# (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\u001b[39;00m\n",
      "File \u001b[0;32m~/PycharmProjects/shapiq/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/shapiq/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/PycharmProjects/shapiq/.venv/lib/python3.11/site-packages/transformers/models/distilbert/modeling_distilbert.py:391\u001b[0m, in \u001b[0;36mDistilBertSdpaAttention.forward\u001b[0;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_heads \u001b[38;5;241m*\u001b[39m dim_per_head)\n\u001b[1;32m    390\u001b[0m q \u001b[38;5;241m=\u001b[39m shape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_lin(query))  \u001b[38;5;66;03m# (bs, n_heads, q_length, dim_per_head)\u001b[39;00m\n\u001b[0;32m--> 391\u001b[0m k \u001b[38;5;241m=\u001b[39m shape(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk_lin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# (bs, n_heads, k_length, dim_per_head)\u001b[39;00m\n\u001b[1;32m    392\u001b[0m v \u001b[38;5;241m=\u001b[39m shape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_lin(value))  \u001b[38;5;66;03m# (bs, n_heads, k_length, dim_per_head)\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# SDPA with memory-efficient backend is broken in torch==2.1.2 when using non-contiguous inputs and a custom\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;66;03m# attn_mask, so we need to call `.contiguous()` here. This was fixed in torch==2.2.0.\u001b[39;00m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;66;03m# Reference: https://github.com/pytorch/pytorch/issues/112577\u001b[39;00m\n",
      "File \u001b[0;32m~/PycharmProjects/shapiq/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/shapiq/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/PycharmProjects/shapiq/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Compute Shapley interactions with the ShapIQ approximator for the game function\n",
    "approximator = shapiq.KernelSHAPIQ(n=game_class.n_players, max_order=2, index=\"k-SII\")\n",
    "sii_values = approximator.approximate(budget=2**game_class.n_players, game=game_class)\n",
    "sii_values.dict_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7641d33a850cdd16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T11:37:37.771196Z",
     "start_time": "2025-09-09T11:37:37.568421Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(): 0.0,\n",
       " (0,): 0.09466196418889895,\n",
       " (1,): 0.2519671876192255,\n",
       " (2,): 0.06853008486648426,\n",
       " (3,): 0.06228182818457484,\n",
       " (4,): 0.1502293735159068,\n",
       " (0, 1): -0.023901337999199194,\n",
       " (0, 2): -0.015578424181147861,\n",
       " (0, 3): 0.013715559286939426,\n",
       " (0, 4): -0.012585067760777176,\n",
       " (1, 2): 0.03777686295041179,\n",
       " (1, 3): -0.07309907222393518,\n",
       " (1, 4): -0.055708334461501,\n",
       " (2, 3): 0.01566102726098756,\n",
       " (2, 4): -0.06081690989708467,\n",
       " (3, 4): 0.022849774106212306}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute Shapley interactions with the ShapIQ approximator for the game object\n",
    "approximator = shapiq.KernelSHAPIQ(n=game_class.n_players, max_order=2, index=\"k-SII\")\n",
    "sii_values = approximator.approximate(budget=2**game_class.n_players, game=game_class)\n",
    "sii_values.dict_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3641f671c8616b",
   "metadata": {},
   "source": [
    "Now let's say we want to do this for a much larger inputs. We can use the `shapiq.SPEX` approximator which is a sparse\n",
    "transform approximator. This approximator is much faster than the KernelSHAPIQ approximator when the number of\n",
    "players is large and can be used for larger inputs. Instead of computing all interactions it computes only the\n",
    "most important ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce6bc4d47530fa2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T15:26:33.768828Z",
     "start_time": "2025-07-11T15:20:33.051503Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 175 players.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game for the full coalition: 0.47598451375961304\n",
      "Game for the empty coalition: 0.0\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "shapiq is a valuable Python library designed for Explainable AI (XAI), focusing specifically on approaches like\n",
    "Shapley values and their extensions. Its core strength lies in providing a unified framework to compute not only individual feature attributions but also sophisticated interaction indices (e.g., Shapley Interaction Index, Banzhaf Index). This allows users to gain deeper insights into how features collaborate or conflict within complex machine learning models, going beyond simple importance scores. A notable weakness stems from the inherent computational complexity of these game-theoretic measures. Calculating exact values, especially for higher-order interactions, is often infeasible, and even approximations can be computationally intensive and time-consuming, particularly for models with many features or large datasets. Despite this, shapiq remains a powerful tool for detailed model inspection.\n",
    "\"\"\"\n",
    "big_game = SentimentClassificationGame(\n",
    "    classifier=classifier, tokenizer=tokenizer, test_sentence=text\n",
    ")\n",
    "print(f\"There are a total of {big_game.n_players} players.\")\n",
    "# To speed up inference, run pipeline with gpu support. Takes ~10 minutes on Mac M1 with MPS.\n",
    "scalable_approximator = shapiq.SPEX(n=big_game.n_players, index=\"SII\")\n",
    "large_sii = scalable_approximator.approximate(budget=32000, game=big_game)\n",
    "print(f\"Game for the full coalition: {game_class(full_coalition)[0]}\")\n",
    "print(f\"Game for the empty coalition: {game_class(empty_coalition)[0]}\")\n",
    "interactions = list(large_sii.dict_values.items())\n",
    "interactions.sort(key=lambda x: abs(x[1]), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81434a652831817",
   "metadata": {},
   "source": "`shapiq.SPEX` identifies interactions between the most sentiment-rich tokens in the paragraph (i.e. *powerful*, *valuable*, *weakness*)"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a09121f781d9be77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T15:26:33.792231Z",
     "start_time": "2025-07-11T15:26:33.790248Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['powerful'], Value: 0.040\n",
      "Tokens: ['valuable', 'powerful'], Value: -0.032\n",
      "Tokens: ['weakness', 'powerful'], Value: 0.026\n",
      "Tokens: ['valuable', 'weakness', 'powerful'], Value: -0.026\n",
      "Tokens: ['weakness'], Value: -0.024\n",
      "Tokens: ['valuable'], Value: 0.021\n",
      "Tokens: ['consuming', 'powerful'], Value: 0.016\n",
      "Tokens: ['insights', 'powerful'], Value: -0.016\n",
      "Tokens: ['valuable', 'weakness'], Value: 0.016\n",
      "Tokens: ['unified', 'powerful'], Value: -0.016\n"
     ]
    }
   ],
   "source": [
    "for inter, value in interactions[:10]:\n",
    "    tokens = [big_game.tokenizer.decode(big_game.tokenized_input[idx]) for idx in inter]\n",
    "    print(f\"Tokens: {tokens}, Value: {value:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
